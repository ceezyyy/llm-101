# LLM 101

## Pretraining

1. Get data from the Internet
2. Tokenization (the compression of the Internet)
3. Neural networking training (predict next token sequences)
4. Inference

Then we get the base model

## Post-training

1. conversation

## References

- [Deep Dive into LLMs like ChatGPT](https://www.youtube.com/watch?v=7xTGNNLPyMI)
- [Tiktokenizer](https://tiktokenizer.vercel.app/?model=cl100k_base)