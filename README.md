# LLM 101

## Pretraining

1. Get data from the Internet
2. Tokenization (the compression of the Internet)
3. Neural networking training (predict next token sequences)
4. Inference

Then we get the base model

## Post-training

1. Supervised Fine-tuning (user-assistant conversations)
2. Reinforcement tranning
3. RLHF

for example 
1. Pretraning: exposistion, background knowledge
2. Supervised finetuning: problem + demonstrated solution, for imitation
3. Reinforcement learning: practice many many times until you reach the correct answer



## References

- [Deep Dive into LLMs like ChatGPT](https://www.youtube.com/watch?v=7xTGNNLPyMI)
- [Tiktokenizer](https://tiktokenizer.vercel.app/?model=cl100k_base)